---
title: "ENTREGA 1"
date: today
date-format: "DD MMMM, YYYY"
author: 
  Mathias
abstract:
    Métodos Estadísticos para la Regresión y la Clasificación
abstractspacing: double
appendix: false
fontfamily: libertine
monofont: inconsolata
monofontoptions: scaled=.95
fontsize: 12pt
geometry: 
  - top=2cm
  - bottom=2cm
  - left=2cm
  - right=2cm
urlcolor: darkblue
highlight-style: arrow
format: 
    pdf:
      toc: true
      number-sections: true
      colorlinks: true
---

# Ejercicio 5: Estudio farmaceutico

## Construir un diagrama de tallos y hojas (extendido) para estos tiempos.

| Stem | Leaf |
|------|------|
| 0    | 9    |
| 1    | 0 0 1 1 2 2 3 3 4 5 5 5 6 7 8 8 8 9 9 9 |
| 2    | 0 0 1 1 2 3 4 6 |

## Construir una tabla de frecuencias y densidades para estos tiempos.

```{r}
# Cargar los tiempos
tiempos <- c(15, 18, 19, 21, 23, 26, 17, 18, 24, 20, 13, 10, 16, 11, 9,
             12, 14, 10, 19, 13, 20, 15, 11, 18, 15, 21, 12, 19, 18, 22)

# Crear los intervalos de 5 unidades
intervalos <- seq(0, 30, by = 5) # avanza de 5 en 5 de 0 a 30

# Tabla de frecuencias
tabla_frecuencias <- cut(tiempos, breaks = intervalos, right = FALSE) # Divide los 
# datos en intervalos ("bins"), para eso se usa cut
frecuencia <- table(tabla_frecuencias) # Crea una tabla que tiene el conteo de cada
# categoria, en este caso intervalos

# Densidades (frecuencia / tamaño del intervalo)
densidad <- prop.table(frecuencia) # Partiendo de la tabla de frecuencias esta 
# funcion se usa para calcular la densidad de frecuencias

# Mostrar la tabla de frecuencias y densidades
tabla_resultado <- data.frame(Intervalo = names(frecuencia),
    Frecuencia = as.vector(frecuencia),
    Densidad = as.vector(densidad))
print(tabla_resultado)

```


## Dibujar un histograma para estos tiempos dividiendo el intervalo [0, 30] en 6 intervalos de longitud 5 en escala frecuencia y en escala densidad.

```{r}

library(ggplot2)

# Crear el dataframe para ggplot
df_tiempos <- data.frame(tiempos) # vector a dataframe

# Histograma en escala de frecuencia
ggplot(df_tiempos, aes(x = tiempos)) +
  geom_histogram(breaks = intervalos, # intervalos para separar
                  fill = "lightblue", # relleno
                  color = "black") + # borde
  labs(title = "Histograma de Frecuencia", x = "Tiempos", y = "Frecuencia") +
  theme_minimal()

# Histograma en escala de densidad
ggplot(df_tiempos, aes(x = tiempos)) +
  geom_histogram(aes(y = after_stat(density)),
   breaks = intervalos,
   fill = "lightgreen", # relleno
   color = "black") + # borde
  geom_density() + # agrega capa de densidad
  labs(title = "Histograma de Densidad", x = "Tiempos", y = "Densidad") +
  theme_minimal()

```

# Ejercicio 7: Tablas de contingencia II

|         | Café | Té | Refresco |
|---------|------|----|----------|
| Hombres | 50 | 30 | 20 |
| Mujeres | 40 | 45 | 15 |

Tabla 1: Tabla de contingencia de preferencias de bebidas por género.

La hipótesis a probar es la siguiente:

Hipótesis nula $H_0$: No existe dependencia entre el género y la preferencia de bebida.
Hipótesis alternativa $H_1$: Existe dependencia entre el género y la preferencia de bebida.

```{r}
# Datos observados
observados <- matrix(c(50, 30, 20, 40, 45, 15), nrow = 2, byrow = TRUE)

# Realizar el test chi-cuadrado
test_chi2 <- chisq.test(observados)

# Resultado del test
test_chi2

```

No hay suficiente evidencia para rechazar la $H_0$.


## Cálculo del estadístico $\chi^2$ sin usar funcion de R:

La prueba $\chi^2$ que da el valor de X-squared, se calcula usando la ecuacion

$$
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

Donde $O_{ij}$ son las frecuencias observadas y $E_{ij}$ son las frecuencias esperadas.

$$
\chi^2 = \frac{(50 - 45)^2}{45} + \frac{(30 - 37.5)^2}{37.5} + \frac{(20 - 17.5)^2}{17.5} + \frac{(40 - 45)^2}{45} + \frac{(45 - 37.5)^2}{37.5} + \frac{(15 - 17.5)^2}{17.5}
$$

Siendo

$$
\chi^2 = \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} + \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} = 0.556 + 1.5 + 0.357 + 0.556 + 1.5 + 0.357 = 4.826
$$

Los grados de libertad se calculan segun los numeros de r = row (fila) y c = column (columna):

$$
\text{gl} = (r - 1)(c - 1) = (2 - 1)(3 - 1) = 2
$$

El valor de la tabla ($\chi^2$) para un nivel de significancia ($\alpha = 0.05$) y 2 grados de libertad es aproximadamente 5.99. Ver tabla $\chi^2$

Como el estadístico calculado ($\chi^2 = 4.826$) es menor que 5.99, no rechazamos la hipótesis nula.


# Ejercicio 4 Descomposición de la varianza II

Sea (X,Y) un vector aleatorio con función de densidad 

$$
f(x, y) = 2, \quad 0 < x < y < 1.
$$

## Calcular $E(X^3 | Y)$

Para calcular la esperanza condicional $E(X^3 | Y)$ dado la funcion de densidad $f(x, y) = 2, \quad 0 < x < y < 1$, tenemos que determinar la distribucion condicional de $X$ dado $Y$.

Primero, necesitamos encontrar la función de $f_Y(y)$. Sabemos que $f(x, y) = 2$ y esta definida en un intervalo.

Calculamos $f_Y(y)$:

$$
f_Y(y) = \int_0^y f(x, y) \, dx = \int_0^y 2 \, dx = 2y.
$$

Ahora usamos la relación, donde $f(x, y) = 2$ y $f_Y(y) = 2y$:

$$
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y}
$$

Calculamos $E(X^3 | Y = y)$, usando la esperanza:

$$
E(X^3 | Y = y) = \int_0^y x^3 f_{X|Y}(x|y) \, dx = \int_0^y x^3 \cdot \frac{1}{y} \, dx = \left[ \frac{x^4}{4} \right]_0^y
$$

Resultando en:

$$
E(X^3 | Y = y) = \frac{1}{y} \cdot \frac{y^4}{4} = \frac{y^3}{4}.
$$

## Descomponer la varianza de la variable $Y$

La varianza de $Y$ se puede descomponer como:

$$
\text{Var}(Y) = E(Y^2) - (E(Y))^2.
$$

Primero, calculamos $E(Y)$:

$$
E(Y) = \int_0^1 y \cdot f_Y(y) \, dy = \int_0^1 y \cdot 2y \, dy = \int_0^1 2y^2 \, dy = \left[ \frac{2y^3}{3} \right]_0^1 = \frac{2}{3}.
$$

Luego, calculamos $E(Y^2)$:

$$
E(Y^2) = \int_0^1 y^2 \cdot f_Y(y) \, dy = \int_0^1 y^2 \cdot 2y \, dy = \int_0^1 2y^3 \, dy = \left[ \frac{2y^4}{4} \right]_0^1 = \frac{1}{2}.
$$

Finalmente, calculamos la varianza:

$$
\text{Var}(Y) = E(Y^2) - (E(Y))^2 = \frac{1}{2} - \left(\frac{2}{3}\right)^2 = \frac{1}{2} - \frac{4}{9} = \frac{1}{18}.
$$

# Ejercicio 7 Normal multivariada

## Hallar la distribución de $Y = X_1 + 2X_2 - 3X_3$

Dado que $Y$ es una combinación lineal de otras variables aleatorias normales, si combinamos variables normales el resultado sera una variable normal. Donde $E[X]$ representa la esperanza, la formula que usamos deriva de las propiedades de la esperanza: La esperanza de una suma es la suma de las esperanzas.

Entonces la media de $Y$ sustituyendo con $\mu = (−1, 1, 0)$:

$$
E[Y] = E[X_1] + 2E[X_2] - 3E[X_3] = -1 + 2(1) - 3(0) = -1 + 2 = 1
$$

Dado que $Y$ es una combinacion lineal de $X_1, X_2 y X_3$ para calcular la varianza podemos usar $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)$. Aplicando la formula calculamos la varianza de $Y$:

$$
\text{Var}(Y) = \text{Var}(X_1) + 2^2\text{Var}(X_2) + (-3)^2\text{Var}(X_3) + 2(1)(2)\text{Cov}(X_1, X_3) + 2(-3)(1)\text{Cov}(X_2, X_3)
$$

Sustituyendo en la fórmula de varianza con los valores de la matriz de covarianzas $\Sigma$:

- $\text{Var}(X_1) = 1$
- $\text{Var}(X_2) = 3$
- $\text{Var}(X_3) = 2$
- $\text{Cov}(X_1, X_2) = 0$
- $\text{Cov}(X_1, X_3) = 1$
- $\text{Cov}(X_2, X_3) = 1$

$$
\text{Var}(Y) = 1 + 2^2(3) + (-3)^2(2) + 2(1)(2)(1) + 2(-3)(1)(1) = 1 + 12 + 18 + 4 - 6 = 29
$$


Siendo la distribución normal de $Y$:
$$
Y \sim N(1, 29)
$$

## Hallar un vector $u = (u_1, u_2)$ tal que las variables aleatorias $X_1$ y $X_1 - (u_1 X_1 + u_2 X_2)$ sean independientes.

Para que $X_1$ y $X_1 - (u_1 X_1 + u_2 X_2)$ sean independientes, su covarianza debe ser cero. La covarianza mide la relación lineal entre dos variables. Si es cero, significa que no existe una relación lineal entre estas.

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = 0
$$

Por las propiedades de la covarianza $\text{Cov}(X1​,aY+bZ)=a⋅\text{Cov}(X1​,Y)+b⋅\text{Cov}(X1​,Z)$ siendo $a, b$ constantes, y $Y$ y $Z$ variables aleatorias. 

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = u_1​⋅\text{Cov}(X_1​,X_1​)+u_2​⋅\text{Cov}(X_1​,X_2​)
$$

Donde $\text{Cov}(X_1​,X_1​) = \text{Var}(X_1) = 1$ y $\text{Cov}(X_1​,X_2​) = 0$ para el caso que $X_1$ y $X_2$ esten incorrelacionadas. Entonces:

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = u_1(1) + u_2(0) = u_1 = 0
$$

Dado que $u_1 = 0$ esto asegura que las variables $X_1​$ y $X_1−(u_1X_1 + u_2X_2)$ sean independientes. El valor de $u_2$ no afecta la independencia de las variables, por lo que puede tomar cualquier número real. Para simplificar tomamos que $u_2 = 1$.

Siendo el vector:
$$
u = (0, 1)
$$

# Ejercicio 4: Sesgo de un Estimador

## Hallar el sesgo de $\hat{\theta}$.

Se sabe que el **error cuadrático medio (ECM)** es $\text{ECM}(\hat{\theta}) = 8$, y se sabe que $P(\hat{\theta} \leq \theta) = 0.8413$. La fórmula del ECM de un estimador es $\text{ECM}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Sesgo}(\hat{\theta})^2$, donde $\text{Sesgo}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta = \mu_{\hat{\theta}} - \theta$

Por otro lado, $P(\hat{\theta} \leq \theta) = 0.8413$ corresponde a un cuantil de la distribucion normal. Buscando en la tabla de distribucion normal con $\text{mu} = 0$ y la probabilidad acumulada $P(Z \leq z) = 0.8413$, entonces $z = 1$.

```{r}

# Probabilidad del cuantil
qnorm(0.8413)

```

$$
P\left(\frac{\hat{\theta} - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}} \leq \frac{\theta - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}}\right) = P(Z \leq 1)
$$

Por lo que podemos obtener la media del estimador a partir de la siguiente igualdad:

$$
\frac{\theta - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}} = 1
$$

Despejamos la media $\mu_{\hat{\theta}}$:
$$
\mu_{\hat{\theta}} = \theta - \sigma_{\hat{\theta}}
$$

Utilizamos esta igualdad para sustituirla en el Sesgo

$$
\text{Sesgo}(\hat{\theta}) = \mu_{\hat{\theta}} - \theta = (\theta - \sigma_{\hat{\theta}}) - \theta = -\sigma_{\hat{\theta}}
$$

Por ultimo utilizamos el ECM para hallar $\sigma_{\hat{\theta}}$, sabiendos que el ECM es 8

$$
\text{ECM}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Sesgo}(\hat{\theta})^2 = \sigma_{\hat{\theta}}^2 + (-\sigma_{\hat{\theta}})^2 = 8
$$

Entonces $8 = 2\sigma_{\hat{\theta}}^2$, despejando el sesgo del estimador es 2

$$
\sigma_{\hat{\theta}}^2 = \frac{8}{2} = 4 = \sigma_{\hat{\theta}} = \sqrt{4}
$$


# Ejercicio 6: Estimador de mínima varianza

Se considera la condición $(C):\frac{\partial}{\partial \theta} \ell(\theta, x) = I(\theta)(T(x) - \theta)$

## Probar que si un estimador $T(x)$ satisface la condición $(C)$, entonces $I(\theta)$ es la información de Fisher de $\theta$.

La **información de Fisher**, $I(\theta)$, tambien puede expresarse como la varianza de la derivada de la log-verosimilitud

$$
I(\theta) = - \mathbb{E}\left[\frac{\partial^2}{\partial \theta^2} \ell(\theta, x)\right] = \text{Var}\left(\frac{\partial}{\partial \theta} \ell(\theta, x)\right)
$$

Dado que la condición $(C)$ que se plantea, le aplicamos la esperanza a ambos lados de la ecuacion. Siendo que $I(\theta)$ es una constante, podemos sacarla fuera de la esperanza

$$
\mathbb{E}\left[\frac{\partial}{\partial \theta} \ell(\theta, x)\right] = I(\theta) \mathbb{E}[T(x) - \theta]
$$

Sabemos que el valor esperado de la derivada de la log-verosimilitud es cero $\mathbb{E}\left[\frac{\partial}{\partial \theta} \ell(\theta, x)\right] = 0$, por lo que $0 = I(\theta)(\mathbb{E}[T(x)] - \theta)$, implicando que $0 = (\mathbb{E}[T(x)] - \theta)$. Entonces si $\mathbb{E}[T(x)] = \theta$, esto muestra que $I(\theta)$ es la información de Fisher.


## Probar que si $\hat{\theta}_{MLE}$ es el estimador de máxima verosimilitud de $\theta$ y $T(x)$ cumple con la condición $(C)$, entonces $T(x) = \hat{\theta}_{MLE}$.

El estimador de máxima verosimilitud, denotado como $\hat{\theta}_{MLE}$, maximiza el valor de $\hat{\theta}$ que hace que la derivada de la log-verosimilitud respecto de $\hat{\theta}$ sea cero $\frac{\partial}{\partial \theta} \ell(\hat{\theta}_{MLE}, x) = 0$

Aplicar la condición $(C)$ en el punto $\hat{\theta}_{MLE}$​, que es donde la derivada de la log-verosimilitud es cero.

$$
\frac{\partial}{\partial \theta} \ell(\hat{\theta}_{MLE}, x) = I(\hat{\theta}_{MLE})(T(x) - \hat{\theta}_{MLE}) = 0
$$

Para estimadores regulares la informacion de Fisher es positva, es decir, $I(\hat{\theta}_{MLE}) > 0$, lo que implica: 
$$
T(x) = \hat{\theta}_{MLE}
$$


# Ejercicio 7 El método delta

## Hallar el estimador de máxima verosimilitud $\hat{a}$ de $a$.

Dada la densidad de probabilidad de $X$:
$$
p(x; a) =
\begin{cases}
(a + 1)x^a & \text{si } 0 < x < 1, \\
0 & \text{si no.}
\end{cases}
$$

La función de verosimilitud es:
$$
L(a; x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} (a + 1) x_i^a = (a + 1)^n \prod_{i=1}^{n} x_i^a
$$

La función de log-verosimilitud es:
$$
\ell(a; x_1, x_2, \dots, x_n) = n \ln(a + 1) + a \sum_{i=1}^{n} \ln(x_i)
$$

Derivamos la log-verosimilitud respecto a $a$ y la igualamos a cero para encontrar el estimador de máxima verosimilitud (MLE):
$$
\frac{\partial}{\partial a} \ell(a; x_1, \dots, x_n) = \frac{n}{a + 1} + \sum_{i=1}^{n} \ln(x_i) = 0
$$

Despejando $a$:
$$
\hat{a} = \frac{-n}{\sum_{i=1}^{n} \ln(x_i)} - 1
$$

```{r}
# Datos de la muestra
muestra <- c(0.56, 0.82, 0.71, 0.87, 0.33, 0.36, 0.93, 0.94, 0.89, 0.42)

# Cálculo del estimador MLE para a
n <- length(muestra)
hat_a <- -n / sum(log(muestra)) - 1
hat_a

```

## Hallar la densidad de Y = − ln(X) y calcular $\text{mu} = \mathbb{E}(Y)$ en función de $a$.

Sabemos que $Y = -\ln(X)$. Usamos la transformación para hallar la densidad de $Y$:
$$
f_Y(y) = (a + 1)(e^{-y})^a \cdot e^{-y} = (a + 1)e^{-(a + 1)y}
$$

Es decir, $Y$ sigue una distribución exponencial con parámetro $a + 1$.

El valor esperado de $Y$ es:
$$
\mu = E(Y) = \frac{1}{a + 1}
$$

## Verificar que $\hat{a} = g(\bar{Y}_n)$, con $g(y) = \frac{1}{y} − 1$ probar que $\hat{a}$ es consistente.

Sabemos que $g(y) = \frac{1}{y} - 1$. Entonces:
$$
\hat{a} = g(\bar{Y}_n) = \frac{1}{\bar{Y}_n} - 1
$$

Dado que $\bar{Y}_n$ converge a $\mu$, el estimador es consistente.

## Usando el desarrollo ˆa − a = g Y n − g(μ) ≈ g′(μ) Y n − μ para n grande

## Se dispone de la siguiente muestra de X

# Ejercicio 10 Información de Fisher

La información de Fisher es la cantidad de información que una variable aleatoria proporciona sobre un parámetro. Esta es la varianza del gradiente del logaritmo de verosimilitud respecto al parámetro, y se expresa como

$$
I(\theta) = -\mathbb{E}\left[\frac{\partial^2}{\partial \theta^2} \log f(X; \theta)\right]
$$

Calcularemos la información de Fisher para distintas distribuciones de propuestas.

## Distribución Bernoulli $X \sim \text{Ber}(p)$

La función de densidad de una variable aleatoria Bernoulli es

$$
f(x; p) = p^x (1-p)^{1-x}, \quad x \in \{0, 1\}
$$

El logaritmo de la función de verosimilitud es:

$$
\log f(x; p) = x \log(p) + (1 - x) \log(1 - p)
$$

La derivada con respecto a  $p$ es:

$$
\frac{\partial}{\partial p} \log f(x; p) = \frac{x}{p} - \frac{1 - x}{1 - p}
$$

La segunda derivada con respecto a $p$ es:

$$
\frac{\partial^2}{\partial p^2} \log f(x; p) = -\frac{x}{p^2} - \frac{1 - x}{(1 - p)^2}
$$

La Información de Fisher es:

$$
\mathcal{I}(p) = -\mathbb{E}\left[ \frac{\partial^2}{\partial p^2} \log f(X; p) \right] = \frac{1}{p(1-p)}
$$

## Distribución Exponencial $X \sim \exp(\lambda)$

La función de densidad de una distribución exponencial es:

$$
f(x; \lambda) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

El logaritmo de la función de verosimilitud es:

$$
\log f(x; \lambda) = \log(\lambda) - \lambda x
$$

La derivada con respecto a $\lambda$ es:

$$
\frac{\partial}{\partial \lambda} \log f(x; \lambda) = \frac{1}{\lambda} - x
$$

La segunda derivada con respecto a $\lambda$ es:

$$
\frac{\partial^2}{\partial \lambda^2} \log f(x; \lambda) = -\frac{1}{\lambda^2}
$$

La Información de Fisher es:

$$
\mathcal{I}(\lambda) = -\mathbb{E}\left[ \frac{\partial^2}{\partial \lambda^2} \log f(X; \lambda) \right] = \frac{1}{\lambda^2}
$$

## Distribución Normal $X \sim N(\mu, 4)$

La función de densidad de una distribución normal es:

$$
f(x; \mu) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

Como $\sigma^2 = 4$, reescribimos la función de densidad como:

$$
f(x; \mu) = \frac{1}{\sqrt{8\pi}} e^{-\frac{(x-\mu)^2}{8}}
$$

El logaritmo de la función de verosimilitud es:

$$
\log f(x; \mu) = -\frac{(x-\mu)^2}{8} - \log(\sqrt{8\pi})
$$

La derivada con respecto a $\mu$ es:

$$
\frac{\partial}{\partial \mu} \log f(x; \mu) = \frac{x - \mu}{4}
$$

La segunda derivada con respecto a $\mu$ es:

$$
\frac{\partial^2}{\partial \mu^2} \log f(x; \mu) = -\frac{1}{4}
$$

La Información de Fisher es:

$$
\mathcal{I}(\mu) = -\mathbb{E}\left[ \frac{\partial^2}{\partial \mu^2} \log f(X; \mu) \right] = \frac{1}{4}
$$



# Ejercicio 2 ¿De tal palo tal astilla?

## Hallar las ecuaciones de las rectas de regresión

La ecuación de la recta de regresión se puede expresar de la siguiente forma

$$
Y = a + bX
$$

Donde 
$Y$ es la altura de las hijas,
$X$ es la altura de las madres,
$b$ es la pendiente de la recta de regresión y
$a$ es la ordenada en el origen

Cálculo de $b$ y $a$

La pendiente $b$ se puede calcular usando la fórmula, sabiendo que $\text{Cov}(X, Y) = \rho.\sigma_X.\sigma_Y$

$$
\beta_X|_Y = \frac{\text{Cov}(X, Y)}{\sigma_X} = \frac{\rho.\sigma_X.\sigma_Y}{\sigma_x^2} = \rho \frac{\sigma_Y}{\sigma_X}
$$

Siendo que $r$ es la correlación (0.5), $\sigma_Y$ es la desviación estándar de $Y$ (0.05) y $\sigma_X$ es la desviación estándar de $X$ (0.05). Sustituimos y obtenemos

$$
b = \beta_X|_Y = 0.5 \cdot \frac{0.05}{0.05} = 0.5
$$

La intersección $a$ se puede calcular usando las siguientes ecuaciones
$b = \beta_{Y \mid X} = \rho \cdot \frac{\sigma_Y}{\sigma_X} = 0.5$ y $a = \mu_Y - b \mu_X$

Siendo 

$$
a = \mu_Y - b\mu_X = 1.63 - 0.5 \cdot 1.63 = 1.63 - 0.815 = 0.815
$$

Por lo que la ecuación de la recta de regresión 

$$
Y = a + bX = 0.815 + 0.5X
$$

## La altura de dos amigas difiere en 5cm. ¿Cuál es tu pronóstico para la diferencia de alturas de las madres?

La diferencia de altura entre dos amigas es de 5 cm (0.05 m). Usando la ecuación de la regresión, la diferencia pronosticada para las alturas de madres es la siguiente

$$
D = b \cdot (X_2 - X_1)
$$

Siendo $D$ la diferencia pronosticada, y $(X_2 - X_1)$ es la diferencia entre alturas. Obtenemos la siguiente ecuacion 

$$
D = 0.5 \cdot 0.05 = 0.025 \text{ m o 2.5 cm}
$$

## Entre las hijas que miden más que la media, ¿qué porcentaje son más bajas que sus madres?

Calcular la altura promedio de las hijas que son más altas que la media (1.63 m) y mas bajas que las madres. Las hijas que miden más que la media siguen una distribución bivariada normal.

Usamos el Z-score que nos proporciona el desvíos estándar. El valor z nos permite comparar directamente una altura con la distribución normal. 

$$
P(Z < z) \text{ donde } z = \frac{Y - \mu_Y}{\sigma_Y}
$$

La distribucion normal es simétrica alrededor de su media, siendo que la mitad de los valores estarán por debajo de la media, y la otra mitad, por encima de ella. Por eso, la probabilidad de que una hija (más alta que la media) sea más baja que su madre es 

$$
P(Z < 0) = 0.5 \quad \text{(debido a la simetría)}
$$

Esto significa que el 50% de las hijas que miden más que la media seran mas bajas que sus madres.

## Una madre mide menos que la media. ¿Cuál es tu pronóstico para la altura de su hija?

Para pronosticar la altura de una hija para una madre que mide menos que la media, ejemplo 1.58 m. Usamos la ecuación de la regresión descripta anteriormente

$$
Y = 0.815 + 0.5 \cdot 1.58 = 0.815 + 0.79 = 1.605 \text{ m}
$$


# Ejercicio 8

En este ejercicio, analizaremos el efecto de la temperatura $t$ en la producción de un material $Y$. Utilizaremos un modelo de regresión lineal para predecir $Y$ en función de $t$ y realizaremos varias pruebas estadísticas.


| $t$  | -5 | -4 | -3 | -2 | -1 | 0  | 1  | 2  | 3  | 4  | 5  |
|------|----|----|----|----|----|----|----|----|----|----|----|
| $Y$  | 1  | 5  | 4  | 7  | 10 | 8  | 9  | 13 | 14 | 13 | 18 |

## Ajustar un modelo de regresión lineal

Primero, ajustamos un modelo de regresión lineal simple:

$$
Y = \beta_0 + \beta_1 t + \epsilon
$$

Donde:
- $Y$ es la variable dependiente (cantidad producida),
- $t$ es la variable independiente (temperatura),
- $\beta_0$ es la intersección,
- $\beta_1$ es la pendiente,
- $\epsilon$ es el término de error.

Usamos R para ajustar el modelo:

```{r}
# Datos
t <- c(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)
Y <- c(1, 5, 4, 7, 10, 8, 9, 13, 14, 13, 18)

# Ajustar modelo de regresión
modelo <- lm(Y ~ t)
summary(modelo)
```

## Probar la hipótesis de que la pendiente es nula

La hipótesis nula se establece como:
$$
H_0​:\beta_1​=0
$$

La hipótesis alternativa es:
$$
H_1:\beta_1 \neq 0
$$

Usaremos un nivel de significancia del $95 %$ $(\alpha = 0.05)$. Para probar la hipótesis, calculamos el valor $p$ correspondiente a $\beta_1$​:

Cálculo del valor $p$

Si el valor $p$ es menor que $\alpha$, rechazamos $H_0$​:

```{r}
# Valor p de la pendiente
valor_p <- summary(modelo)$coefficients[2, 4]
valor_p
```

Rechazamos la hipótesis nula y concluimos que la pendiente no es nula.

## Intervalo de confianza para el valor esperado de $Y$

El intervalo de confianza para el valor esperado de $Y$ se calcula como:

$$
\hat{Y} \pm t_{\alpha/2, n-2} \cdot SE(\hat{Y})
$$

Donde:

- $\hat{Y}$ es el valor predicho para $Y$ en $t=3$,
- $t_{\alpha/2, n-2}$ es el valor crítico de la distribución $t$ con $n-2$ grados de libertad,
- $SE(\hat{Y})$ es el error estándar de la predicción.

```{r}
# Predicción para t = 3
nueva_dato <- data.frame(t = 3)
prediccion <- predict(modelo, nueva_dato, interval = "confidence", level = 0.95)
prediccion

```

## Intervalo de confianza para una futura $Y$

El intervalo de confianza para una futura observación de $Y$ se calcula como:

$$
\hat{Y} \pm t_{\alpha/2, n-2} \cdot SE_{total}
$$

Donde:

$$
SE_{total} = \sqrt{SE(\hat{Y})^2 + \sigma^2}
$$

Aquí, $SE_{total}$ representa el error estándar total, y $\sigma$ es la desviación estándar del modelo.

```{r}
# Intervalo de predicción para una nueva observación
prediccion_futura <- predict(modelo, nueva_dato, interval = "prediction", level = 0.95)
prediccion_futura

```