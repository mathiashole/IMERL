---
title: "ENTREGA 1"
date: today
date-format: "DD MMMM, YYYY"
author: 
  Mathias
abstract:
    Métodos Estadísticos para la Regresión y la Clasificación
abstractspacing: double
appendix: false
fontfamily: libertine
monofont: inconsolata
monofontoptions: scaled=.95
fontsize: 12pt
geometry: 
  - top=2cm
  - bottom=2cm
  - left=2cm
  - right=2cm
urlcolor: darkblue
highlight-style: arrow
format: 
    pdf:
      toc: true
      number-sections: true
      colorlinks: true
---

# Ejercicio 5: Estudio farmaceutico

## Construir un diagrama de tallos y hojas (extendido) para estos tiempos.

| Stem | Leaf |
|------|------|
| 0    | 9    |
| 1    | 0 0 1 1 2 2 3 3 4 5 5 5 6 7 8 8 8 9 9 9 |
| 2    | 0 0 1 1 2 3 4 6 |

## Construir una tabla de frecuencias y densidades para estos tiempos.

```{r}
# Cargar los tiempos
tiempos <- c(15, 18, 19, 21, 23, 26, 17, 18, 24, 20, 13, 10, 16, 11, 9,
             12, 14, 10, 19, 13, 20, 15, 11, 18, 15, 21, 12, 19, 18, 22)

# Crear los intervalos de 5 unidades
intervalos <- seq(0, 30, by = 5) # avanza de 5 en 5 de 0 a 30

# Tabla de frecuencias
tabla_frecuencias <- cut(tiempos, breaks = intervalos, right = FALSE) # Divide los 
# datos en intervalos ("bins"), para eso se usa cut
frecuencia <- table(tabla_frecuencias) # Crea una tabla que tiene el conteo de cada
# categoria, en este caso intervalos

# Densidades (frecuencia / tamaño del intervalo)
densidad <- prop.table(frecuencia) # Partiendo de la tabla de frecuencias esta 
# funcion se usa para calcular la densidad de frecuencias

# Mostrar la tabla de frecuencias y densidades
tabla_resultado <- data.frame(Intervalo = names(frecuencia),
    Frecuencia = as.vector(frecuencia),
    Densidad = as.vector(densidad))
print(tabla_resultado)

```


## Dibujar un histograma para estos tiempos dividiendo el intervalo [0, 30] en 6 intervalos de longitud 5 en escala frecuencia y en escala densidad.

```{r}

library(ggplot2)

# Crear el dataframe para ggplot
df_tiempos <- data.frame(tiempos) # vector a dataframe

# Histograma en escala de frecuencia
ggplot(df_tiempos, aes(x = tiempos)) +
  geom_histogram(breaks = intervalos, # intervalos para separar
                  fill = "lightblue", # relleno
                  color = "black") + # borde
  labs(title = "Histograma de Frecuencia", x = "Tiempos", y = "Frecuencia") +
  theme_minimal()

# Histograma en escala de densidad
ggplot(df_tiempos, aes(x = tiempos)) +
  geom_histogram(aes(y = after_stat(density)),
   breaks = intervalos,
   fill = "lightgreen", # relleno
   color = "black") + # borde
  geom_density() + # agrega capa de densidad
  labs(title = "Histograma de Densidad", x = "Tiempos", y = "Densidad") +
  theme_minimal()

```

# Ejercicio 7: Tablas de contingencia II

|         | Café | Té | Refresco |
|---------|------|----|----------|
| Hombres | 50 | 30 | 20 |
| Mujeres | 40 | 45 | 15 |

Tabla 1: Tabla de contingencia de preferencias de bebidas por género.

La hipótesis a probar es la siguiente:

Hipótesis nula $H_0$: No existe dependencia entre el género y la preferencia de bebida.
Hipótesis alternativa $H_1$: Existe dependencia entre el género y la preferencia de bebida.

```{r}
# Datos observados
observados <- matrix(c(50, 30, 20, 40, 45, 15), nrow = 2, byrow = TRUE)

# Realizar el test chi-cuadrado
test_chi2 <- chisq.test(observados)

# Resultado del test
test_chi2

```

No hay suficiente evidencia para rechazar la $H_0$.


## Cálculo del estadístico $\chi^2$ sin usar funcion de R:

La prueba $\chi^2$ que da el valor de X-squared, se calcula usando la ecuacion

$$
\chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

Donde $O_{ij}$ son las frecuencias observadas y $E_{ij}$ son las frecuencias esperadas.

$$
\chi^2 = \frac{(50 - 45)^2}{45} + \frac{(30 - 37.5)^2}{37.5} + \frac{(20 - 17.5)^2}{17.5} + \frac{(40 - 45)^2}{45} + \frac{(45 - 37.5)^2}{37.5} + \frac{(15 - 17.5)^2}{17.5}
$$

Siendo

$$
\chi^2 = \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} + \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} = 0.556 + 1.5 + 0.357 + 0.556 + 1.5 + 0.357 = 4.826
$$

Simplificamos:

$$
\chi^2 = \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} + \frac{25}{45} + \frac{56.25}{37.5} + \frac{6.25}{17.5} = 0.556 + 1.5 + 0.357 + 0.556 + 1.5 + 0.357 = 4.826
$$

Los grados de libertad se calculan segun los numeros de r = row (fila) y c = column (columna):

$$
\text{gl} = (r - 1)(c - 1) = (2 - 1)(3 - 1) = 2
$$

El valor de la tabla ($\chi^2$) para un nivel de significancia ($\alpha = 0.05$) y 2 grados de libertad es aproximadamente 5.99. Ver tabla $\chi^2$

Como el estadístico calculado ($\chi^2 = 4.826$) es menor que 5.99, no rechazamos la hipótesis nula.


# Ejercicio 4 Descomposición de la varianza II

Sea (X,Y) un vector aleatorio con función de densidad 

$$
f(x, y) = 2, \quad 0 < x < y < 1.
$$

## Calcular $E(X^3 | Y)$

Para calcular la esperanza condicional $E(X^3 | Y)$ dado la funcion de densidad $f(x, y) = 2, \quad 0 < x < y < 1$, tenemos que determinar la distribucion condicional de $X$ dado $Y$.

Primero, necesitamos encontrar la función de $f_Y(y)$. Sabemos que $f(x, y) = 2$ y esta definida en un intervalo.

Calculamos $f_Y(y)$:

$$
f_Y(y) = \int_0^y f(x, y) \, dx = \int_0^y 2 \, dx = 2y.
$$

Ahora usamos la relación, donde $f(x, y) = 2$ y $f_Y(y) = 2y$:

$$
f_{X|Y}(x|y) = \frac{f(x, y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y}
$$

Calculamos $E(X^3 | Y = y)$, usando la esperanza:

$$
E(X^3 | Y = y) = \int_0^y x^3 f_{X|Y}(x|y) \, dx = \int_0^y x^3 \cdot \frac{1}{y} \, dx = \left[ \frac{x^4}{4} \right]_0^y
$$

Resultando en:

$$
E(X^3 | Y = y) = \frac{1}{y} \cdot \frac{y^4}{4} = \frac{y^3}{4}.
$$

## Descomponer la varianza de la variable $Y$

La varianza de $Y$ se puede descomponer como:

$$
\text{Var}(Y) = E(Y^2) - (E(Y))^2.
$$

Primero, calculamos $E(Y)$:

$$
E(Y) = \int_0^1 y \cdot f_Y(y) \, dy = \int_0^1 y \cdot 2y \, dy = \int_0^1 2y^2 \, dy = \left[ \frac{2y^3}{3} \right]_0^1 = \frac{2}{3}.
$$

Luego, calculamos $E(Y^2)$:

$$
E(Y^2) = \int_0^1 y^2 \cdot f_Y(y) \, dy = \int_0^1 y^2 \cdot 2y \, dy = \int_0^1 2y^3 \, dy = \left[ \frac{2y^4}{4} \right]_0^1 = \frac{1}{2}.
$$

Finalmente, calculamos la varianza:

$$
\text{Var}(Y) = E(Y^2) - (E(Y))^2 = \frac{1}{2} - \left(\frac{2}{3}\right)^2 = \frac{1}{2} - \frac{4}{9} = \frac{1}{18}.
$$

# Ejercicio 7 Normal multivariada

## Hallar la distribución de $Y = X_1 + 2X_2 - 3X_3$

Dado que $Y$ es una combinación lineal de otras variables aleatorias normales, si combinamos variables normales el resultado sera una variable normal. Donde $E[X]$ representa la esperanza, la formula que usamos deriva de las propiedades de la esperanza: La esperanza de una suma es la suma de las esperanzas.

Entonces la media de $Y$ sustituyendo con $\mu = (−1, 1, 0)$:

$$
E[Y] = E[X_1] + 2E[X_2] - 3E[X_3] = -1 + 2(1) - 3(0) = -1 + 2 = 1
$$

Dado que $Y$ es una combinacion lineal de $X_1, X_2 y X_3$ para calcular la varianza podemos usar $Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)$. Aplicando la formula calculamos la varianza de $Y$:

$$
\text{Var}(Y) = \text{Var}(X_1) + 2^2\text{Var}(X_2) + (-3)^2\text{Var}(X_3) + 2(1)(2)\text{Cov}(X_1, X_3) + 2(-3)(1)\text{Cov}(X_2, X_3)
$$

Sustituyendo en la fórmula de varianza con los valores de la matriz de covarianzas $\Sigma$:

- $\text{Var}(X_1) = 1$
- $\text{Var}(X_2) = 3$
- $\text{Var}(X_3) = 2$
- $\text{Cov}(X_1, X_2) = 0$
- $\text{Cov}(X_1, X_3) = 1$
- $\text{Cov}(X_2, X_3) = 1$

$$
\text{Var}(Y) = 1 + 2^2(3) + (-3)^2(2) + 2(1)(2)(1) + 2(-3)(1)(1) = 1 + 12 + 18 + 4 - 6 = 29
$$


Siendo la distribución normal de $Y$:
$$
Y \sim N(1, 29)
$$

## Hallar un vector $u = (u_1, u_2)$ tal que las variables aleatorias $X_1$ y $X_1 - (u_1 X_1 + u_2 X_2)$ sean independientes.

Para que $X_1$ y $X_1 - (u_1 X_1 + u_2 X_2)$ sean independientes, su covarianza debe ser cero. La covarianza mide la relación lineal entre dos variables. Si es cero, significa que no existe una relación lineal entre estas.

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = 0
$$

Por las propiedades de la covarianza $\text{Cov}(X1​,aY+bZ)=a⋅\text{Cov}(X1​,Y)+b⋅\text{Cov}(X1​,Z)$ siendo $a, b$ constantes, y $Y$ y $Z$ variables aleatorias. 

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = u_1​⋅\text{Cov}(X_1​,X_1​)+u_2​⋅\text{Cov}(X_1​,X_2​)
$$

Donde $\text{Cov}(X_1​,X_1​) = \text{Var}(X_1) = 1$ y $\text{Cov}(X_1​,X_2​) = 0$ para el caso que $X_1$ y $X_2$ esten incorrelacionadas. Entonces:

$$
\text{Cov}(X_1, u_1 X_1 + u_2 X_2) = u_1(1) + u_2(0) = u_1 = 0
$$

Dado que $u_1 = 0$ esto asegura que las variables $X_1​$ y $X_1−(u_1X_1 + u_2X_2)$ sean independientes. El valor de $u_2$ no afecta la independencia de las variables, por lo que puede tomar cualquier número real. Para simplificar tomamos que $u_2 = 1$.

Siendo el vector:
$$
u = (0, 1)
$$

# Ejercicio 4: Sesgo de un Estimador

## Hallar el sesgo de $\hat{\theta}$.

Se sabe que el **error cuadrático medio (ECM)** es $\text{ECM}(\hat{\theta}) = 8$, y se sabe que $P(\hat{\theta} \leq \theta) = 0.8413$. La fórmula del ECM de un estimador es $\text{ECM}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Sesgo}(\hat{\theta})^2$, donde $\text{Sesgo}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta = \mu_{\hat{\theta}} - \theta$

Por otro lado, $P(\hat{\theta} \leq \theta) = 0.8413$ corresponde a un cuantil de la distribucion normal. Buscando en la tabla de distribucion normal con $\text{mu} = 0$ y la probabilidad acumulada $P(Z \leq z) = 0.8413$, entonces $z = 1$.

```{r}

# Probabilidad del cuantil
qnorm(0.8413)

```

$$
P\left(\frac{\hat{\theta} - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}} \leq \frac{\theta - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}}\right) = P(Z \leq 1)
$$

Por lo que podemos obtener la media del estimador a partir de la siguiente igualdad:

$$
\frac{\theta - \mu_{\hat{\theta}}}{\sigma_{\hat{\theta}}} = 1
$$

Despejamos la media $\mu_{\hat{\theta}}$:
$$
\mu_{\hat{\theta}} = \theta - \sigma_{\hat{\theta}}
$$

Utilizamos esta igualdad para sustituirla en el Sesgo

$$
\text{Sesgo}(\hat{\theta}) = \mu_{\hat{\theta}} - \theta = (\theta - \sigma_{\hat{\theta}}) - \theta = -\sigma_{\hat{\theta}}
$$

Por ultimo utilizamos el ECM para hallar $\sigma_{\hat{\theta}}$, sabiendos que el ECM es 8

$$
\text{ECM}(\hat{\theta}) = \text{Var}(\hat{\theta}) + \text{Sesgo}(\hat{\theta})^2 = \sigma_{\hat{\theta}}^2 + (-\sigma_{\hat{\theta}})^2 = 8
$$

Entonces $8 = 2\sigma_{\hat{\theta}}^2$, despejando el sesgo del estimador es 2

$$
\sigma_{\hat{\theta}}^2 = \frac{8}{2} = 4 = \sigma_{\hat{\theta}} = \sqrt{4}
$$


# Ejercicio 6: Estimador de mínima varianza

Se considera la condición $(C):\frac{\partial}{\partial \theta} \ell(\theta, x) = I(\theta)(T(x) - \theta)$

## Probar que si un estimador $T(x)$ satisface la condición $(C)$, entonces $I(\theta)$ es la información de Fisher de $\theta$.

La **información de Fisher**, $I(\theta)$, tambien puede expresarse como la varianza de la derivada de la log-verosimilitud

$$
I(\theta) = - \mathbb{E}\left[\frac{\partial^2}{\partial \theta^2} \ell(\theta, x)\right] = \text{Var}\left(\frac{\partial}{\partial \theta} \ell(\theta, x)\right)
$$

Dado que la condición $(C)$ que se plantea, le aplicamos la esperanza a ambos lados de la ecuacion. Siendo que $I(\theta)$ es una constante, podemos sacarla fuera de la esperanza

$$
\mathbb{E}\left[\frac{\partial}{\partial \theta} \ell(\theta, x)\right] = I(\theta) \mathbb{E}[T(x) - \theta]
$$

Sabemos que el valor esperado de la derivada de la log-verosimilitud es cero $\mathbb{E}\left[\frac{\partial}{\partial \theta} \ell(\theta, x)\right] = 0$, por lo que $0 = I(\theta)(\mathbb{E}[T(x)] - \theta)$, implicando que $0 = (\mathbb{E}[T(x)] - \theta)$. Entonces si $\mathbb{E}[T(x)] = \theta$, esto muestra que $I(\theta)$ es la información de Fisher.


## Probar que si $\hat{\theta}_{MLE}$ es el estimador de máxima verosimilitud de $\theta$ y $T(x)$ cumple con la condición $(C)$, entonces $T(x) = \hat{\theta}_{MLE}$.

El estimador de máxima verosimilitud, denotado como $\hat{\theta}_{MLE}$, maximiza el valor de $\hat{\theta}$ que hace que la derivada de la log-verosimilitud respecto de $\hat{\theta}$ sea cero $\frac{\partial}{\partial \theta} \ell(\hat{\theta}_{MLE}, x) = 0$

Aplicar la condición $(C)$ en el punto $\hat{\theta}_{MLE}$​, que es donde la derivada de la log-verosimilitud es cero.

$$
\frac{\partial}{\partial \theta} \ell(\hat{\theta}_{MLE}, x) = I(\hat{\theta}_{MLE})(T(x) - \hat{\theta}_{MLE}) = 0
$$

Para estimadores regulares la informacion de Fisher es positva, es decir, $I(\hat{\theta}_{MLE}) > 0$, lo que implica: 
$$
T(x) = \hat{\theta}_{MLE}
$$


# Ejercicio 7 El método delta

## Hallar el estimador de máxima verosimilitud $\hat{a}$ de $a$.

Dada la densidad de probabilidad de $X$:
$$
p(x; a) =
\begin{cases}
(a + 1)x^a & \text{si } 0 < x < 1, \\
0 & \text{si no.}
\end{cases}
$$

La función de verosimilitud es:
$$
L(a; x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} (a + 1) x_i^a = (a + 1)^n \prod_{i=1}^{n} x_i^a
$$

La función de log-verosimilitud es:
$$
\ell(a; x_1, x_2, \dots, x_n) = n \ln(a + 1) + a \sum_{i=1}^{n} \ln(x_i)
$$

Derivamos la log-verosimilitud respecto a $a$ y la igualamos a cero para encontrar el estimador de máxima verosimilitud (MLE):
$$
\frac{\partial}{\partial a} \ell(a; x_1, \dots, x_n) = \frac{n}{a + 1} + \sum_{i=1}^{n} \ln(x_i) = 0
$$

Despejando $a$:
$$
\hat{a} = \frac{-n}{\sum_{i=1}^{n} \ln(x_i)} - 1
$$

```{r}
# Datos de la muestra
muestra <- c(0.56, 0.82, 0.71, 0.87, 0.33, 0.36, 0.93, 0.94, 0.89, 0.42)

# Cálculo del estimador MLE para a
n <- length(muestra)
hat_a <- -n / sum(log(muestra)) - 1
hat_a

```

## Hallar la densidad de Y = − ln(X) y calcular $\text{mu} = \mathbb{E}(Y)$ en función de $a$.

Sabemos que $Y = -\ln(X)$. Usamos la transformación para hallar la densidad de $Y$:
$$
f_Y(y) = (a + 1)(e^{-y})^a \cdot e^{-y} = (a + 1)e^{-(a + 1)y}
$$

Es decir, $Y$ sigue una distribución exponencial con parámetro $a + 1$.

El valor esperado de $Y$ es:
$$
\mu = E(Y) = \frac{1}{a + 1}
$$

## Verificar que $\hat{a} = g(\bar{Y}_n)$, con $g(y) = \frac{1}{y} − 1$ probar que $\hat{a}$ es consistente.

Sabemos que $g(y) = \frac{1}{y} - 1$. Entonces:
$$
\hat{a} = g(\bar{Y}_n) = \frac{1}{\bar{Y}_n} - 1
$$

Dado que $\bar{Y}_n$ converge a $\mu$, el estimador es consistente.

## Usando el desarrollo ˆa − a = g Y n − g(μ) ≈ g′(μ) Y n − μ para n grande

## Se dispone de la siguiente muestra de X